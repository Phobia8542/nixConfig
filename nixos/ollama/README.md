# Ollama / LLM Module

How I run models locally from the terminal or in my browser (**open-webui**)

Includes AMD and Nvidia variants